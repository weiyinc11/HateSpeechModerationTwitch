# -*- coding: utf-8 -*-
"""Toxigen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b2jJsLKbPpUa-lpo03Ay-_SOTE38FEk_
"""

pip install pandas

import pandas as pd

# Load the dataset
file_path = 'toxigen.csv'

# Option 1: Load entire dataset (ensure system has enough RAM)
data = pd.read_csv(file_path)

print(len(data))
print(data.head)

import matplotlib.pyplot as plt

# Plot distribution of roberta scores
plt.figure(figsize=(10, 6))
plt.hist(data['roberta_prediction'], bins=50, alpha=0.7, color='blue', label='RoBERTa Score Distribution')
plt.xlabel('RoBERTa Prediction Score')
plt.ylabel('Frequency')
plt.title('Distribution of RoBERTa Scores')
plt.legend()
plt.show()

plt.figure(figsize=(10, 6))
plt.hist(data['roberta_prediction'], bins=50, alpha=0.7, color='blue', label='RoBERTa Score Distribution')
plt.yscale('log')  # Apply logarithmic scale
plt.xlabel('RoBERTa Prediction Score')
plt.ylabel('Log Frequency')
plt.title('Distribution of RoBERTa Scores (Log Scale)')
plt.legend()
plt.show()

# Define thresholds
# data = data[data['generation'].apply(lambda x: len(str(x)) < 500)]
hateful_threshold = 0.8
non_hateful_threshold = 0.2

# Select hateful and non-hateful samples
hateful_samples = data[(data['roberta_prediction'] >= hateful_threshold) & (data['prompt_label'] == 1)].sample(n=20000, random_state=42)
non_hateful_samples = data[(data['roberta_prediction'] <= non_hateful_threshold) & (data['prompt_label'] == 0)].sample(n=20000, random_state=42)

# Combine sampled data
balanced_samples = pd.concat([hateful_samples, non_hateful_samples])

# Save the result (optional)
balanced_samples.to_csv("balanced_toxigen_sample.csv", index=False)
print(non_hateful_samples.head)

non_hateful_samples.to_csv("NonHatetoxigen.csv",index=False)

data = hateful_samples.copy()
print("Column Names:")
print(data.columns)

print(len(data))
# Specify the column for which you want unique values
column_name = 'group'  # Replace with the actual column name

# Check if the column exists
if column_name in data.columns:
    unique_values = data[column_name].unique()
    print(f"\nUnique values in column '{column_name}':")
    print(unique_values)
else:
    print(f"Column '{column_name}' not found in the dataset.")
print(data.head)

# Count the number of points in each group
group_counts = data['group'].value_counts()

# Print the counts for each group
print("Number of points in each group:")
print(group_counts)

mini = data['roberta_prediction'].min()
print(mini)

# Filter rows where 'prompt_label' = 1 and 'roberta_prediction' >= 0.5
filtered_rows = data.copy()
filtered_rows.rename(columns={'generation': 'text'}, inplace=True)
# Count the number of such rows
count = filtered_rows.shape[0]
# Save the filtered data to a CSV file
filtered_rows.to_csv('toxigen_data_level1.csv', index=False)

print("Filtered data has been saved to 'filtered_data.csv'.")
# Print the count
print(f"Number of rows where 'prompt_label' = 1 and 'roberta_prediction' >= 0.95: {count}")

group_counts = filtered_rows.groupby('group').size()

# Print the counts for each group
print("Group counts where 'prompt_label' = 1 and 'roberta_prediction' >= 0.5:")
print(group_counts)

# Define groups for datasets
import zipfile


groups = {
    "Disability": ["mental_dis", "physical_dis"],
    "Misogyny": ["women"],
    "Race_Religion_and_Ethnicity": [
        "asian", "black", "chinese", "jewish", "latino", "mexican",
        "middle_east", "muslim", "native_american"
    ],
    "Sexuality_Sex_or_Gender": ["lgbtq"]
}

# Create datasets based on groups
datasets = {}
for category, group_values in groups.items():
    datasets[category] = filtered_rows[filtered_rows['group'].isin(group_values)]

# Save each dataset to a CSV file
csv_files = []
for category, df in datasets.items():
    print(len(df))
    file_name = f"{category.replace(' ', '_')}.csv"
    df.to_csv(file_name, index=False)
    csv_files.append(file_name)

# Create a ZIP file to store the CSVs
zip_file_name = "Level2_Toxigen.zip"
with zipfile.ZipFile(zip_file_name, 'w') as zipf:
    for file in csv_files:
        zipf.write(file)

print(f"All datasets saved into the ZIP file: {zip_file_name}")

import zipfile


groups = {
    "Mental_Dis": ["mental_dis"],
    "Physical_Dis": ["physical_dis"],
    "Jewish" : ["jewish"],
    "Black" : ["black"],
    "Muslim" : ["muslim"]
}

# Create datasets based on groups
datasets = {}
for category, group_values in groups.items():
    datasets[category] = filtered_rows[filtered_rows['group'].isin(group_values)]

# Save each dataset to a CSV file
csv_files = []
for category, df in datasets.items():
    print(len(df))
    file_name = f"{category.replace(' ', '_')}.csv"
    df.to_csv(file_name, index=False)
    csv_files.append(file_name)

# Create a ZIP file to store the CSVs
zip_file_name = "Level3_Toxigen.zip"
with zipfile.ZipFile(zip_file_name, 'w') as zipf:
    for file in csv_files:
        zipf.write(file)

print(f"All datasets saved into the ZIP file: {zip_file_name}")

