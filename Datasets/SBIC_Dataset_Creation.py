# -*- coding: utf-8 -*-
"""SBIC_processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sjMKE8qKxVLlaOTeqlZjEq9_RBnuiu3h

Each line in the file contains the following fields (in order):
- _whoTarget_: group vs. individual target
- _intentYN_: was the intent behind the statement to offend
- _sexYN_: is the post a sexual or lewd reference
- _sexReason_: free text explanations of what is sexual
- _offensiveYN_: could the post be offensive to anyone
- _annotatorGender_: gender of the MTurk worker
- _annotatorMinority_: whether the MTurk worker identifies as a minority
- _sexPhrase_: part of the post that references something sexual
- _speakerMinorityYN_: whether the speaker was part of the same minority group that's being targeted
- _WorkerId_: hashed version of the MTurk workerId
- _HITId_: id that uniquely identifies each post
- _annotatorPolitics_: political leaning of the MTurk worker
- _annotatorRace_: race of the MTurk worker
- _annotatorAge_: age of the MTurk worker
- _post_: post that was annotated
- _targetMinority_: demographic group targeted
- _targetCategory_: high-level category of the demographic group(s) targeted
- _targetStereotype_: implied statement
- _dataSource_: source of the post (`t/...`: means Twitter, `r/...`: means a subreddit)
"""

import pandas as pd
import numpy as np
import json

# Load the uploaded training dataset CSV file
file_path = '/content/SBIC.v2.trn.csv'
df = pd.read_csv(file_path)

# Define the text and class fields
textFields = ['targetMinority', 'targetCategory', 'targetStereotype']
classFields = ['whoTarget', 'intentYN', 'sexYN', 'offensiveYN']
df = df.dropna(subset=["offensiveYN"])

# Aggregation rules for both cases
aggDict = {c: lambda x: sorted(filter(lambda x: x, set(x))) for c in textFields}
aggDict.update({c: lambda x: np.mean(x) for c in classFields})

# Fill NaN values in text fields
df[textFields] = df[textFields].fillna("")

# --- OLD CODE ---
gDf_old = df.groupby("post", as_index=False).agg(aggDict)
gDf_old["hasBiasedImplication"] = (gDf_old["targetStereotype"].apply(len) == 0).astype(int)
gDf_old[textFields] = gDf_old[textFields].apply(lambda c: c.apply(json.dumps))

# --- NEW CODE (With at least 2 annotators) ---
valid_posts = df.groupby('post')['WorkerId'].nunique()
valid_posts = valid_posts[valid_posts >= 2].index

filtered_df = df[df['post'].isin(valid_posts)]
filtered_df[textFields] = filtered_df[textFields].fillna("")

gDf_new = filtered_df.groupby("post", as_index=False).agg(aggDict)
gDf_new["hasBiasedImplication"] = (gDf_new["targetStereotype"].apply(len) == 0).astype(int)
gDf_new[textFields] = gDf_new[textFields].apply(lambda c: c.apply(json.dumps))

print(len(gDf_old))
print(len(gDf_new))
# Save the outputs to CSV files
output_old = 'aggregated_posts_old.csv'
output_new = 'aggregated_posts_new.csv'

gDf_old.to_csv(output_old, index=False)
gDf_new.to_csv(output_new, index=False)

results = {
    "OffensiveSum == 0": len(gDf_new[gDf_new['offensiveYN'] == 0]),
    "OffensiveSum != 0": len(gDf_new[gDf_new['offensiveYN'] != 0]),
    "OffensiveSum >= 0.3": len(gDf_new[gDf_new['offensiveYN'] >= 0.3]),
    "OffensiveSum >= 0.5": len(gDf_new[gDf_new['offensiveYN'] >= 0.5]),
    "OffensiveSum >= 0.7": len(gDf_new[gDf_new['offensiveYN'] >= 0.7]),
    "OffensiveSum == 1.0": len(gDf_new[gDf_new['offensiveYN'] >= 1.0])
}

results

import pandas as pd
import numpy as np
import html
import re

# Set a random seed for reproducibility
np.random.seed(42)

# Step 1: Randomly select 20k examples (same set every time)
sampled_df = gDf_new.sample(n=20000, random_state=42)
sampled_df.rename(columns={'post': 'text'}, inplace=True)
# Step 2: Remove all user IDs mentioned in the 'post' column (e.g., remove @username)
def remove_user_ids(post):
    return re.sub(r'@\S+', '', post)

sampled_df['text'] = sampled_df['text'].apply(remove_user_ids)

# Step 3: Convert HTML entities to their respective emojis in the 'post' column
sampled_df['text'] = sampled_df['text'].apply(html.unescape)

# Step 4: Remove duplicate examples based on the 'post' column
sampled_df = sampled_df.drop_duplicates(subset=['text'])
# print(len(sampled_df))
# Step 5: Get stats for NaN values in the 'offensiveYN' column
num_nan_offensive_yn = sampled_df['offensiveYN'].isna().sum()

# Output the stats
print(f"Number of examples containing NaN values in 'OffensiveYN' column: {num_nan_offensive_yn}")

# Optional: Save the processed dataset to a new CSV file
processed_output = 'sbic_final.csv'
sampled_df.to_csv(processed_output, index=False)

# Identify the remaining examples (those not in sampled_df)
remaining_df = gDf_new[~gDf_new.index.isin(sampled_df.index)]
remaining_output = 'remaining_gdf_new.csv'
remaining_df.to_csv(remaining_output, index=False)
# print(len(remaining_df))
# Output stats
print(f"Number of examples in the sampled dataset: {len(sampled_df)}")
print(f"Number of examples in the remaining dataset: {len(remaining_df)}")

print(sampled_df)

# import pandas as pd
# import os

# def divide_csv_into_chunks(file_path, chunk_size, output_dir):
#     """
#     Divides a CSV file into smaller chunks of a specified size.

#     Args:
#         file_path (str): Path to the input CSV file.
#         chunk_size (int): Number of rows per chunk.
#         output_dir (str): Directory to save the chunked files.

#     Returns:
#         None
#     """
#     # Ensure the output directory exists
#     os.makedirs(output_dir, exist_ok=True)

#     # Read the CSV in chunks and write to separate files
#     chunk_number = 1
#     for chunk in pd.read_csv(file_path, chunksize=chunk_size):
#         output_file = os.path.join(output_dir, f"sbic_chunk_{chunk_number}.csv")
#         chunk.to_csv(output_file, index=False)
#         print(f"Chunk {chunk_number} saved to {output_file}")
#         chunk_number += 1

# # Example usage
# file_path = "sbic_final.csv"  # Path to your large CSV
# chunk_size = 10000  # Number of rows per chunk
# output_dir = "chunks"  # Directory to save chunked files

# divide_csv_into_chunks(file_path, chunk_size, output_dir)

import pandas as pd

# Load the datasets (replace with your actual file paths)
data = pd.read_csv('/content/sbic_final.csv')
data = data.drop(columns=['whoTarget', 'intentYN', 'hasBiasedImplication'])

# Filter rows where offensiveYN is non-zero
data = data[data['offensiveYN'] >= 0.5]

# Convert targetCategory and targetMinority to lists if stored as strings
data['targetCategory'] = data['targetCategory'].apply(lambda x: eval(x) if isinstance(x, str) else [])
data['targetMinority'] = data['targetMinority'].apply(lambda x: eval(x) if isinstance(x, str) else [])

# Define the mapping function for standardizing target minorities
def standardize_minority(minority):
    mapping = {
        "Jewish Folks": ["jewish folks", "jewish people", "jews", "hebrew", "holocaust survivors", "holocaust victims",
                         "all groups targeted by nazis", "jewish victims", "holocaust survivers", "holocaust survivors/jews"],
        "Black Folks": ["black folks", "blacks", "black people", "black africans", "african americans", "black lives matter supporters",
                        "afro-americans", "black victims of racial abuse", "light skinned black folks", "black jew"],
        "Muslim Folks": ["muslim folks", "muslims", "islamic folks", "islamic people", "arabic folks", "muslim women", "islamics",
                         "islam", "middle eastern", "middle-eastern folks", "arabian", "muslim kids"],
        "Asian Folks": ["asian folks", "asians", "chinese", "japanese", "korean", "asian people", "east asians", "southeast asians",
                        "indian folks", "asian women", "asian folks, indians", "asian folks, japanese","brown folks"],
        "Latino/Latina Folks": ["latino/latina folks", "hispanic folks", "mexican", "latinos", "latinas", "mexican folks",                        #brown folks come in asian or latino
                                "spanish-speaking people", "hispanics"],
        "LGBT Community": ["lgbt", "LGBT", "lgbtq+", "gay men", "lesbian women", "trans women", "trans men", "bisexual men",
                           "queer people", "lgbtq+ folks", "lgbt youth", "gender fluid folks", "non-binary folks", "genderqueer",
                           "gender neutral", "trans folk", "non-binary", "gay folks", "all lgtb folks"],
        "Immigrants and Refugees": ["immigrants", "refugees", "foreigners", "syrian refugees", "afghan refugees", "asylum seekers",
                                    "displaced persons", "palestinian refugees", "refugees fleeing conflict", "immigrants, africans",
                                    "immigrants, children of immigrants", "immigrants, mexicans"],
        "Physically Diabled Folks": ["physically disabled folks","people with physical illness/disorder","deaf people", "blind people", "the handicapped", "speech impediment"],
        "Mentally Diabled Folks": [ "mentally disabled folks", "people with autism", "autistic people",
                           "autistic children", "folks with mental illness/disorder"],
        "Women": ["women", "feminists", "female assault victims", "lesbian women", "trans women", "bisexual women", "all feminists",
                  "feminist women", "females", "transgender women", "pregnant folks", "single mothers", "women's who've had abortions"],
        "Victims of Abuse": ["assault victims", "rape victims", "molestation victims", "sexual abuse victims", "domestic abuse victims",
                             "incest victims", "child molestation victims", "child sexual abuse victims", "harassment victims",
                             "pedophilia victims", "sex abuse victims", "sexual assault victims", "kidnap victims", "bullying victims"],
        "Homeless People": ["homeless people", "homeless folks", "people living on the streets", "the homeless", "homeless victim"],
        "Poor Folks": ["poor folks", "people in poverty", "low-income families", "poverty-stricken individuals", "rural people"],
        "Religious Folks": ["christians", "muslims", "jews", "hindu folks", "buddhists", "religious people in general",
                            "spiritual people", "people of faith", "all religious folks"],
        "Cancer Patients": ["cancer victims", "cancer patients", "cancer survivors", "folks with cancer", "terminally ill children"],
        "Non-Whites": ["non-whites", "all non-whites", "any non-white race", "racial minorities", "minority folks",
                       "minorities in general", "asian folks, latino/latina folks, non-whites"],
        "Elderly People": ["seniors", "old folks", "elderly people", "old ladies"],
        "Children": ["kids", "children", "young kids", "young girls", "young boys", "school kids", "teenagers", "child brides",
                     "child rape victims", "assault victims, child rape victims", "child molestation victims",
                     "children who have been sexually assaulted"],
        "Indigenous People": ["native american/first nation folks", "aboriginal", "indigenous people", "eskimos", "maori folk"],
        "Police": ["police", "cops", "police officers", "police shooting victims"],
        "Military": ["veterans", "marines", "war vets", "military"],
        "Transgender Folks": ["trans folks", "trans women", "trans men", "non-binary folks"],
        "Addiction-Related": ["drugged victims", "drug addicts", "alcoholics", "people with anorexia"],
        "Mental Illness": ["people with mental illness", "folks with mental illness", "depressed folks"],
        # Add additional mappings if needed
    }

    for standard, variations in mapping.items():
        if minority in variations:
            return standard
    return minority  # Return original if no match

# Standardize `targetMinority` column
data['targetMinority'] = data['targetMinority'].apply(lambda minorities: [standardize_minority(minority) for minority in minorities])

# Part where SSG doesnt include women

import os
import zipfile

# Define mappings for each Twitch subfilter
twitch_subfilters = {
    "Disability": ["Physically Diabled Folks","Mentally Diabled Folks","Mental Illness"],
    "Sexuality, Sex, or Gender": ["LGBT Community", "Transgender Folks"],
    "Misogyny": ["Women"],
    "Race, Ethnicity, or Religion": ["Black Folks", "Jewish Folks", "Muslim Folks", "Asian Folks","Latino/Latina Folks", "Indigenous People", "Religious Folks","Non-Whites"],

}

# Filter data by minority and save subsets
def filter_by_minority(data, minority_set):
    return data[data['targetMinority'].apply(lambda x: any(minority in x for minority in minority_set))]

# Create folder for Level 2 subsets
output_folder = "Level2_sbic"
os.makedirs(output_folder, exist_ok=True)  # Create the folder if it doesn't exist

# Save each subset into the folder
for subfilter, categories in twitch_subfilters.items():
    subset = filter_by_minority(data, set(categories))

    if len(subset) > 0:
        subset_filename = f"{subfilter.replace(' ', '_').replace('-', '').lower()}_subset.csv"
        subset_filepath = os.path.join(output_folder, subset_filename)
        subset.to_csv(subset_filepath, index=False)
        print(f"Saved subset for {subfilter} with size {len(subset)}")
    else:
        print(f"No data matched for {subfilter}")

# Zip the Level2_sbic folder
zip_filename = "Level2_sbic.zip"
with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
    for root, _, files in os.walk(output_folder):
        for file in files:
            filepath = os.path.join(root, file)
            arcname = os.path.relpath(filepath, output_folder)  # Preserve relative folder structure
            zipf.write(filepath, arcname)

print(f"All subsets saved and zipped into {zip_filename}")

import os
import zipfile

# Define mappings for each Twitch subfilter
twitch_subfilters = {
    "Disability": ["Physically Diabled Folks","Mentally Diabled Folks","Mental Illness"],
    "Sexuality, Sex, or Gender": ["LGBT Community", "Transgender Folks", "Women"],
    "Misogyny": ["Women"],
    "Race, Ethnicity, or Religion": ["Black Folks", "Jewish Folks", "Muslim Folks", "Asian Folks","Latino/Latina Folks", "Indigenous People", "Religious Folks","Non-Whites"],

}

# Filter data by minority and save subsets
def filter_by_minority(data, minority_set):
    return data[data['targetMinority'].apply(lambda x: any(minority in x for minority in minority_set))]

# Create folder for Level 2 subsets
output_folder = "Level2_sbic"
os.makedirs(output_folder, exist_ok=True)  # Create the folder if it doesn't exist

# Save each subset into the folder
for subfilter, categories in twitch_subfilters.items():
    subset = filter_by_minority(data, set(categories))

    if len(subset) > 0:
        subset_filename = f"{subfilter.replace(' ', '_').replace('-', '').lower()}_subset.csv"
        subset_filepath = os.path.join(output_folder, subset_filename)
        subset.to_csv(subset_filepath, index=False)
        print(f"Saved subset for {subfilter} with size {len(subset)}")
    else:
        print(f"No data matched for {subfilter}")

# Zip the Level2_sbic folder
zip_filename = "Level2_sbic.zip"
with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
    for root, _, files in os.walk(output_folder):
        for file in files:
            filepath = os.path.join(root, file)
            arcname = os.path.relpath(filepath, output_folder)  # Preserve relative folder structure
            zipf.write(filepath, arcname)

print(f"All subsets saved and zipped into {zip_filename}")

category_mapping = {
    "Disability": {
        "Disabled Folks": ["Physically Diabled Folks"],
        "Mental Illness": ["Mentally Diabled Folks","Mental Illness"]
        # "Cancer Patients": ["Cancer Patients"]
    },

    "Race, Ethnicity, or Religion (RER)": {
        "Black Folks": ["Black Folks"],
        "Asian Folks": ["Asian Folks"],
        "Muslim Folks": ["Muslim Folks"],
        "Jewish Folks": ["Jewish Folks"],
        "Latino/Latina Folks": ["Latino/Latina Folks"],
        "Indigenous People": ["Indigenous People"],
        "Immigrants and Refugees": ["Immigrants and Refugees"],
        "Religious Folks": ["Religious Folks"],
        "Non-Whites": ["Non-Whites"]
    }
}

# Function to filter data by Level 3 categories
def filter_by_category(data, subcategories):
    return data[data['targetMinority'].apply(lambda x: any(minority in subcategories for minority in x))]

# Create a folder for Level 3 subsets
level3_folder = "Level3_sbic"
os.makedirs(level3_folder, exist_ok=True)

# Iterate over category_mapping and save subsets
for main_category, subcategories_dict in category_mapping.items():
    for subcategory, terms in subcategories_dict.items():
        subset = filter_by_category(data, set(terms))

        if len(subset) > 0:
            filename = f"{main_category}_{subcategory}.csv".replace(" ", "_").replace("/", "_").lower()
            filepath = os.path.join(level3_folder, filename)
            subset.to_csv(filepath, index=False)
            print(f"Saved subset for {main_category} - {subcategory} with size {len(subset)} to {filepath}")
        else:
            print(f"No data matched for {main_category} - {subcategory}")

# Zip the Level3_sbic folder
zip_filename = "Level3_sbic.zip"
with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
    for root, _, files in os.walk(level3_folder):
        for file in files:
            filepath = os.path.join(root, file)
            arcname = os.path.relpath(filepath, level3_folder)
            zipf.write(filepath, arcname)

print(f"All Level 3 subsets saved and zipped into {zip_filename}")

